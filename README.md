# Service Virtualization

## Background

Modern software systems tend to be comprised of numerous components that implements certain functionalities. A component may depend on one or more other components' functionalities to perform the function of its own. In turn, a component may be required to provide its functionality to other components. In other words, the producer-consumer relationships are established between the various components of a given system. The functionalities that a component provides for other components to invoke, or consume, are called services, and the component is called a service provider. A component that invokes the services is called a service consumer.

In order to make services usable by a multitude of consumers, a service implements a well defined service contract in a form of an API. API's can differ greatly depending on the service functionality, intended consumption, security concerns and many other factors. As of this writing, REST based API's are most popular in the cloud environments but SOAP based web service remain in heavy use as well. There are also a plethora of other protocols for use in both cloud, on-premises and hybrid architectures.

Provisioning a system that consists of many different service providers and consumers can be a daunting task. If operating in a continuous delivery model where modification to the software are small and frequent, while each modification triggers a complete cycle of building, deploying and testing of the change, making the gamut of the service available may turn out to be impossible. It may also not be feasible to reliably test software as the number of the components grows large.

Service virtualization is an approach where a fake service facade, also known as a mock or stub, is provide in place of the real service provider. The virtualized service mirror the real API, however it can only serve pre-defined service request with the pre-defined responses. This dramatically reduces the cost of orchestrating service dependencies during deployment and testing of the component or a set of component under test.

## Benefits of Service Virtualization
As mentioned above, virtualized services reduce the cost of satisfying service dependencies during software development. Often they make it possible to have a full set of dependencies, although virtually, where certain integration, functional or end-to-end test scenarios were not possible to create at all. The second main reason to virtualize dependency services after cost is control that you can exert over the behavior of the service during testing. Here are some situation where virtualized service can help, especially if the service is external to your organization which is becoming more and more common.

1. A real service may be unavailable. In this case the continuous test(s) will fail. A virtualized service can be made infinitely better accessible and allow avoiding unnecessary continuous process interruptions.
2. A real live service may experience high loads and become slow to respond. This results in your tests to run more slowly too, thus reducing the throughput of your CI pipeline.
3. Speaking of slow response, your system under test may behave differently depending on the response time of the dependency service, especially if such a service invokes transient dependency services. A virtualized service is under your control and its response time can be control too. This perhaps creates an opportunity to test the system under test as the response time of the dependency service varies.
4. A real service costs money to use. In the API economy, a service could charge for the API calls or limit the number of API calls allowed within a certain timeframe. In the case of the former, the cost of running an extensive test will quickly add up. In the latter, your test will be very slow as they will quickly exhaust the API call quota. The cost of running a virtualized service is negligible compared that of a real service and doesn't have the API call limits. Of course, if testing how the system under test handles the API limit situation is the object of the test, a virtualized service can simulate that too, but now on purpose.
5. Many services provides live data that constantly changes, e.g. a Twitter user's timeline. As a result, it is impossible to fully test the code under test that makes these API calls because there is no way to verify if the service response is handled correctly.
6. Often, a service provider moves very quickly to deploy the new features in their service. One would hope that the changes are backward compatible and don't cause any compatibility issues for the service consumers. But in a fast paced economy, situations that prove otherwise occur more often than we want to believe. Even if the the provider realizes they cause some consumer an issue and take steps to fix the problem, it may take time and the disruption to your company already happened. A virtualized service behavior pattern is fixed and it creates a buffer that will buy you time to react to any external disruption like this.
7. Last but not least, a virtualized service may simulate a service that doesn't yet exist. What if it will take the next 3 to 6 months for the service provider to develop and/or deploy the service your software depends on to production. If you wait until then with your development, you may loose the edge against the competition. However, if the service contract is available now, a virtualized service can be quickly and cheaply created base on this service contract, so your development and testing can start immediately.

## Use Cases

### Weather API
My [OpenWeatherMap-VirtSrvc](https://github.com/aseriy/OpenWeatherMap-VirtSrvc) on GitHub illustrates how multiple projects that use different technologies can leverage a virtualized service. These project have achieved a dramatic increase in the test code coverage by writing tests that exercise the that make API calls to the weather service. This was not possible without the virtualized service. For example, because the weather constantly changes, without the virtualized service, the real service would always return the different temperature, humidity or wind direction and speed for either the current conditions or forecast. The virtualized service allows serving the same values at any time and make assertion in the tests.
It also enable to use multiple test data sets on demand. For example, one data set simulates winter time in Boston, the second simulates summer in Austin, while the third simulates spring in Paris. This is important if visual elements such as rain, sunshine or thunderstorm icons are testing in a mobile application.
One of the projects also demonstrates a data-driven test where the same API call(s) are made with a wide range of inputs in a loop. This is important, for example, if the application handles temperature in a wide range correctly, such as from -40 C to +40C with the 0.5 C increment. Here is another example. The mobile application has to correctly fit the temperature reading into a specific visual area using a non-fixed width font. A massive automated test like this may help uncover an incorrect rendering.

### Social Media
Applications leveraging social media are very popular. However, interaction with these service isn't easy to test continually for two reasons. Let's take Twitter for example.
* Twitter timeline of a give user changes as the account followed by the user can tweet at any time. A test may be expecting a particular twit from a particular Twitter user.
* The mobile application under test may be tweeting a particular message, send direct messages to a particular Twitter user, or follow the certain users. If the tests exercising the associated Twitter API calls are run repeatedly, the Twitter account used for testing will probably be suspended as such activities will be detected and classified as SPAM. A virtualized service can set all the initial conditions for the tests every time the tests are run. Tests can be repeated as much as needed without using a real Twitter account.
* As of this writing, Twitter API limit most invocations to 180 call every 15 minutes. Some calls are much more restricted. A fairly simple test case, if repeated a few times, can easily exhausts this API call quota within a few seconds. A virtualized service eliminates this problem.
* Now, we really want to repeatedly test how our application handles the situation when the quota is exhausted. With the real service, we would need to keep making API calls until the quota is exhausted just to initialized our test. This seems to be a huge waste of API calls. A virtualized service can simulate that on demand.

Similar situation can be explored with most of the social media service such as LinkedIn, Facebook, etc.

### Geolocation
Many application, especially mobile, require to work with the current geolocation. Let's look at just a couple of scenarios.

One, the application locates restaurant, concerts, movies and public transportation the nearby area. The objective of the end-to-end test is to ensure that the correct establishments and events are displayed on the correct map. This requires to virtually place the device running the application in a series of geographic locations and then verify the results of the search. The only way to achieve this is to spoof the location of the device with the pre-defined latitude/longitude values. A virtualized service allows to do this.

Two, the application calculates driving directions. In this case, not only the geolocation has to be spoofed in order to run a test in a controlled environment, but the geolocation must be changing continuously. Since the device isn't moving, using a real service will not be possible, but a virtual service can be used to simulate the changing geolocation.

### Street, Train and Air Traffic
The system under test is a mobile application helping travelers plan their routes. Let's say Bob is traveling from London, UK to Providence, RI. He's on a plane from Heathrow to Boston Logan Airport. If the plane lands on time at 5:20 pm, the application should advise Bob to take the MBTA Silver Line from the terminal E to South Station and take the 7:15 pm train to Providence. However, if the plane is late, given that the 7:15 is the last train to Providence for the day, Bob should be on the 8:00 Greyhound bus. Yet, if the flight is getting into Boston early, there twp other travelers Sue and Beth on the plane from Paris that has landed at 4:45 pm, and they are going to Providence too. If they all share an Uber ride, they'll be at their destinations earlier and at a lower cost if either train or bus.

Simulating a test case like this would never be possible using the live data feeds. However, replacing the live data feeds with virtualized service makes it possible.

### Time
Today, most applications need the network time reading in order to operate. There is obviously the most trivial use cases where the current time is being displayed, perhaps in the different time zones. The task becomes much more complex if the test scenario spans a considerable length of real time. For example, a wearable device monitors and logs a patient's glucose level throughout a week. The readings are sent to the cloud for real-time analysis, and the cloud service sends instructions back to the device to administer insuline infusion based on the data analysis. The challenge in this case is that a system decision on Friday is based on the data collected since Monday, a test case would require five days to run. This would completely put such a test case out of reach ... unless the test environment is controlled to operate on a sped up clock, i.e. the environment clock is 24 times faster than reality, almost like a time-lapse video.
This can be achieved by substituting the NTP, Network Time Protocol, that the entire test environment synchs to with a virtualized service that pretends to be a regular clock except it runs 24 hours of test time in 1 hour of real time.
There are certain limitations as to how much faster a test environment can be run based on the real-time processes run by the various system components, but typically there is plenty of idle time built into a system that can be "squeezed" during testing.

### Internet of Things
Since the previous section talks about a wearable device, it's worth discussing some challenges with testing IoT related applications. The challenge is two-fold.

When testing an IoT device, the device needs to communicate with its service counterpart. Depending on the type of such cloud service, it could expensive to provision. A virtualized service can be a much more feasible route.

On the flip side, let's say the system under test is the cloud service that collects data from a large number of devices and sends data back. The ability to control the data the devices send to the service can make a world of difference. The other challenge is to have a large number of devices for the purpose of a test. Whichever protocol is being used, CoAP, XMPP, REST or MQTT, virtualizing the IoT devices solves both problems. First, it allows scaling up the virtual devices farm easily and cheaply. Second, it provides the opportunity to drive the "devices" in a predictable pattern to facilitate meaningful test cases.

For example, a cable company has 1 million set top boxes in a certain service area. These STB's constantly collect diagnostics information and log it to the cloud. The cloud service then runs analytics related to the STB inventory, firmware version, network quality, etc. It then makes real-time decisions on upgrades, dispatching technicians to the area, and other activities. End-to-end testing of this service may not be possible unless 1 million STB's are virtualized.

### Data Sources
Finally, if provisioning compute for testing isn't challenging enough, there is data store, i.e. databases, both SQL and no-SQL, and file storage. If data store is an integral part of the system, then its availability and initial state are paramount in a well designed test cases. Let's take a look at the following example. A system for collecting and analyzing water supply quality is using 3 different data stores - Oracle for customer information and billing, MySQL for water meter readings, and MongoDB for water quality data. The new feature of the analytics application is being introduced and requires modification to data schemas in all three databases.
In order to thoroughly test the migration process, the following process needs to be repeated:

1. Create the instances of all three data sources with the data consisten across all three.
2. Run the migration process.
3. Verify the integrity of the data sources after the migration.
4. Run application regression tests.
4. Destroy the test data source instances.

Most likely, cloning the live databases for this purpose isn't going to be practical. Usually the large size of databases like these is the culprit. There may also be legal restrictions on testing with real customer data. Virtualizing the data sources could be the solution. A virtualized data source may only need to include a limited type and amount of data to suit the required testing. This allows provisioning test instances quickly and controlling the test point precisely for the purpose of asserting data values during the tests.